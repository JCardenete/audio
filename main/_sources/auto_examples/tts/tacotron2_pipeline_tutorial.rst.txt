
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/tts/tacotron2_pipeline_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tts_tacotron2_pipeline_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tts_tacotron2_pipeline_tutorial.py:


Text-to-speech with torchaudio Tacotron2
========================================

**Author** `Yao-Yuan Yang <https://github.com/yangarbiter>`__,
`Moto Hira <moto@fb.com>`__

.. GENERATED FROM PYTHON SOURCE LINES 11-46

Overview
--------

This tutorial shows how to build text-to-speech pipeline, using the
pretrained Tacotron2 in torchaudio.

The text-to-speech pipeline goes as follows:

1. Text preprocessing

   First, the input text is encoded into a list of symbols. In this
   tutorial, we will use English characters and phonemes as the symbols.

2. Spectrogram generation

   From the encoded text, a spectrogram is generated. We use ``Tacotron2``
   model for this.

3. Time-domain conversion

   The last step is converting the spectrogram into the waveform. The
   process to generate speech from spectrogram is also called Vocoder.
   In this tutorial, three different vocoders are used,
   `WaveRNN <https://pytorch.org/audio/stable/models/wavernn.html>`__,
   `Griffin-Lim <https://pytorch.org/audio/stable/transforms.html#griffinlim>`__,
   and
   `Nvidia's WaveGlow <https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/>`__.


The following figure illustrates the whole process.

.. image:: https://download.pytorch.org/torchaudio/tutorial-assets/tacotron2_tts_pipeline.png

All the related components are bundled in :py:func:`torchaudio.pipelines.Tacotron2TTSBundle`,
but this tutorial will also cover the process under the hood.

.. GENERATED FROM PYTHON SOURCE LINES 48-55

Preparation
-----------

First, we install the necessary dependencies. In addition to
``torchaudio``, ``DeepPhonemizer`` is required to perform phoneme-based
encoding.


.. GENERATED FROM PYTHON SOURCE LINES 55-76

.. code-block:: default


    # When running this example in notebook, install DeepPhonemizer
    # !pip3 install deep_phonemizer

    import torch
    import torchaudio
    import matplotlib
    import matplotlib.pyplot as plt

    import IPython

    matplotlib.rcParams['figure.figsize'] = [16.0, 4.8]

    torch.random.manual_seed(0)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(torch.__version__)
    print(torchaudio.__version__)
    print(device)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    1.11.0.dev20211105+cpu
    0.11.0.dev20211105+cpu
    cpu




.. GENERATED FROM PYTHON SOURCE LINES 77-80

Text Processing
---------------


.. GENERATED FROM PYTHON SOURCE LINES 83-101

Character-based encoding
~~~~~~~~~~~~~~~~~~~~~~~~

In this section, we will go through how the character-based encoding
works.

Since the pre-trained Tacotron2 model expects specific set of symbol
tables, the same functionalities available in ``torchaudio``. This
section is more for the explanation of the basis of encoding.

Firstly, we define the set of symbols. For example, we can use
``'_-!\'(),.:;? abcdefghijklmnopqrstuvwxyz'``. Then, we will map the
each character of the input text into the index of the corresponding
symbol in the table.

The following is an example of such processing. In the example, symbols
that are not in the table are ignored.


.. GENERATED FROM PYTHON SOURCE LINES 101-114

.. code-block:: default


    symbols = '_-!\'(),.:;? abcdefghijklmnopqrstuvwxyz'
    look_up = {s: i for i, s in enumerate(symbols)}
    symbols = set(symbols)

    def text_to_sequence(text):
      text = text.lower()
      return [look_up[s] for s in text if s in symbols]

    text = "Hello world! Text to speech!"
    print(text_to_sequence(text))






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [19, 16, 23, 23, 26, 11, 34, 26, 29, 23, 15, 2, 11, 31, 16, 35, 31, 11, 31, 26, 11, 30, 27, 16, 16, 14, 19, 2]




.. GENERATED FROM PYTHON SOURCE LINES 115-120

As mentioned in the above, the symbol table and indices must match
what the pretrained Tacotron2 model expects. ``torchaudio`` provides the
transform along with the pretrained model. For example, you can
instantiate and use such transform as follow.


.. GENERATED FROM PYTHON SOURCE LINES 120-130

.. code-block:: default


    processor = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()

    text = "Hello world! Text to speech!"
    processed, lengths = processor(text)

    print(processed)
    print(lengths)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    tensor([[19, 16, 23, 23, 26, 11, 34, 26, 29, 23, 15,  2, 11, 31, 16, 35, 31, 11,
             31, 26, 11, 30, 27, 16, 16, 14, 19,  2]])
    tensor([28], dtype=torch.int32)




.. GENERATED FROM PYTHON SOURCE LINES 131-138

The ``processor`` object takes either a text or list of texts as inputs.
When a list of texts are provided, the returned ``lengths`` variable
represents the valid length of each processed tokens in the output
batch.

The intermediate representation can be retrieved as follow.


.. GENERATED FROM PYTHON SOURCE LINES 138-142

.. code-block:: default


    print([processor.tokens[i] for i in processed[0, :lengths[0]]])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 't', 'e', 'x', 't', ' ', 't', 'o', ' ', 's', 'p', 'e', 'e', 'c', 'h', '!']




.. GENERATED FROM PYTHON SOURCE LINES 143-162

Phoneme-based encoding
~~~~~~~~~~~~~~~~~~~~~~

Phoneme-based encoding is similar to character-based encoding, but it
uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme)
model.

The detail of the G2P model is out of scope of this tutorial, we will
just look at what the conversion looks like.

Similar to the case of character-based encoding, the encoding process is
expected to match what a pretrained Tacotron2 model is trained on.
``torchaudio`` has an interface to create the process.

The following code illustrates how to make and use the process. Behind
the scene, a G2P model is created using ``DeepPhonemizer`` package, and
the pretrained weights published by the author of ``DeepPhonemizer`` is
fetched.


.. GENERATED FROM PYTHON SOURCE LINES 162-175

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH

    processor = bundle.get_text_processor()

    text = "Hello world! Text to speech!"
    with torch.inference_mode():
      processed, lengths = processor(text)

    print(processed)
    print(lengths)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      0%|          | 0.00/63.6M [00:00<?, ?B/s]      0%|          | 56.0k/63.6M [00:00<03:21, 331kB/s]      0%|          | 240k/63.6M [00:00<01:26, 764kB/s]       1%|          | 616k/63.6M [00:00<00:46, 1.41MB/s]      3%|2         | 1.59M/63.6M [00:00<00:20, 3.17MB/s]      6%|6         | 4.12M/63.6M [00:00<00:08, 7.39MB/s]     10%|#         | 6.63M/63.6M [00:01<00:06, 9.95MB/s]     15%|#5        | 9.57M/63.6M [00:01<00:04, 13.5MB/s]     17%|#7        | 11.1M/63.6M [00:01<00:04, 13.1MB/s]     22%|##1       | 14.0M/63.6M [00:01<00:03, 15.8MB/s]     25%|##4       | 15.6M/63.6M [00:01<00:03, 15.0MB/s]     29%|##8       | 18.4M/63.6M [00:01<00:02, 17.0MB/s]     32%|###1      | 20.1M/63.6M [00:01<00:02, 15.9MB/s]     35%|###4      | 22.2M/63.6M [00:01<00:02, 17.5MB/s]     38%|###8      | 24.3M/63.6M [00:02<00:02, 17.5MB/s]     41%|####      | 26.0M/63.6M [00:02<00:02, 16.8MB/s]     43%|####3     | 27.6M/63.6M [00:02<00:02, 16.6MB/s]     47%|####7     | 30.2M/63.6M [00:02<00:01, 17.8MB/s]     50%|#####     | 31.9M/63.6M [00:02<00:01, 17.0MB/s]     53%|#####2    | 33.5M/63.6M [00:02<00:01, 16.8MB/s]     57%|#####6    | 36.1M/63.6M [00:02<00:01, 18.0MB/s]     59%|#####9    | 37.8M/63.6M [00:02<00:01, 17.1MB/s]     62%|######1   | 39.4M/63.6M [00:03<00:01, 16.8MB/s]     66%|######5   | 42.0M/63.6M [00:03<00:01, 18.1MB/s]     69%|######8   | 43.7M/63.6M [00:03<00:01, 17.2MB/s]     71%|#######1  | 45.3M/63.6M [00:03<00:01, 16.9MB/s]     75%|#######5  | 47.9M/63.6M [00:03<00:00, 18.0MB/s]     78%|#######7  | 49.6M/63.6M [00:03<00:00, 17.1MB/s]     80%|########  | 51.2M/63.6M [00:03<00:00, 16.9MB/s]     84%|########4 | 53.7M/63.6M [00:03<00:00, 18.0MB/s]     87%|########7 | 55.4M/63.6M [00:03<00:00, 17.2MB/s]     90%|########9 | 57.1M/63.6M [00:04<00:00, 16.9MB/s]     94%|#########3| 59.6M/63.6M [00:04<00:00, 18.0MB/s]     96%|#########6| 61.3M/63.6M [00:04<00:00, 17.2MB/s]     99%|#########8| 63.0M/63.6M [00:04<00:00, 16.9MB/s]    100%|##########| 63.6M/63.6M [00:04<00:00, 15.0MB/s]
    tensor([[54, 20, 65, 69, 11, 92, 44, 65, 38,  2, 11, 81, 40, 64, 79, 81, 11, 81,
             20, 11, 79, 77, 59, 37,  2]])
    tensor([25], dtype=torch.int32)




.. GENERATED FROM PYTHON SOURCE LINES 176-181

Notice that the encoded values are different from the example of
character-based encoding.

The intermediate representation looks like the following.


.. GENERATED FROM PYTHON SOURCE LINES 181-185

.. code-block:: default


    print([processor.tokens[i] for i in processed[0, :lengths[0]]])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ['HH', 'AH', 'L', 'OW', ' ', 'W', 'ER', 'L', 'D', '!', ' ', 'T', 'EH', 'K', 'S', 'T', ' ', 'T', 'AH', ' ', 'S', 'P', 'IY', 'CH', '!']




.. GENERATED FROM PYTHON SOURCE LINES 186-202

Spectrogram Generation
----------------------

``Tacotron2`` is the model we use to generate spectrogram from the
encoded text. For the detail of the model, please refer to `the
paper <https://arxiv.org/abs/1712.05884>`__.

It is easy to instantiate a Tacotron2 model with pretrained weight,
however, note that the input to Tacotron2 models need to be processed
by the matching text processor.

:py:func:`torchaudio.pipelines.Tacotron2TTSBundle` bundles the matching
models and processors together so that it is easy to create the pipeline.

For the available bundles, and its usage, please refer to :py:mod:`torchaudio.pipelines`.


.. GENERATED FROM PYTHON SOURCE LINES 202-219

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH
    processor = bundle.get_text_processor()
    tacotron2 = bundle.get_tacotron2().to(device)

    text = "Hello world! Text to speech!"

    with torch.inference_mode():
      processed, lengths = processor(text)
      processed = processed.to(device)
      lengths = lengths.to(device)
      spec, _, _ = tacotron2.infer(processed, lengths)


    plt.imshow(spec[0].cpu().detach())





.. image-sg:: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_001.png
   :alt: tacotron2 pipeline tutorial
   :srcset: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading: "https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth" to /root/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_wavernn_ljspeech.pth
      0%|          | 0.00/107M [00:00<?, ?B/s]      7%|7         | 7.77M/107M [00:00<00:01, 81.4MB/s]     14%|#4        | 15.5M/107M [00:00<00:01, 78.5MB/s]     21%|##1       | 23.0M/107M [00:00<00:01, 76.4MB/s]     28%|##8       | 30.3M/107M [00:00<00:01, 76.4MB/s]     35%|###5      | 37.8M/107M [00:00<00:00, 76.7MB/s]     42%|####1     | 45.1M/107M [00:00<00:00, 74.8MB/s]     49%|####8     | 52.3M/107M [00:00<00:00, 74.4MB/s]     55%|#####5    | 59.4M/107M [00:00<00:00, 72.8MB/s]     62%|######1   | 66.3M/107M [00:00<00:00, 62.0MB/s]     69%|######8   | 73.7M/107M [00:01<00:00, 66.0MB/s]     75%|#######5  | 81.1M/107M [00:01<00:00, 69.3MB/s]     82%|########1 | 88.0M/107M [00:01<00:00, 67.9MB/s]     89%|########9 | 95.8M/107M [00:01<00:00, 71.8MB/s]     97%|#########6| 104M/107M [00:01<00:00, 74.9MB/s]     100%|##########| 107M/107M [00:01<00:00, 72.4MB/s]

    <matplotlib.image.AxesImage object at 0x7fabc127d550>



.. GENERATED FROM PYTHON SOURCE LINES 220-223

Note that ``Tacotron2.infer`` method perfoms multinomial sampling,
therefor, the process of generating the spectrogram incurs randomness.


.. GENERATED FROM PYTHON SOURCE LINES 223-233

.. code-block:: default


    fig, ax = plt.subplots(3, 1, figsize=(16, 4.3 * 3))
    for i in range(3):
      with torch.inference_mode():
        spec, spec_lengths, _ = tacotron2.infer(processed, lengths)
      print(spec[0].shape)
      ax[i].imshow(spec[0].cpu().detach())
    plt.show()





.. image-sg:: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_002.png
   :alt: tacotron2 pipeline tutorial
   :srcset: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    torch.Size([80, 155])
    torch.Size([80, 167])
    torch.Size([80, 164])




.. GENERATED FROM PYTHON SOURCE LINES 234-243

Waveform Generation
-------------------

Once the spectrogram is generated, the last process is to recover the
waveform from the spectrogram.

``torchaudio`` provides vocoders based on ``GriffinLim`` and
``WaveRNN``.


.. GENERATED FROM PYTHON SOURCE LINES 246-252

WaveRNN
~~~~~~~

Continuing from the previous section, we can instantiate the matching
WaveRNN model from the same bundle.


.. GENERATED FROM PYTHON SOURCE LINES 252-276

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH

    processor = bundle.get_text_processor()
    tacotron2 = bundle.get_tacotron2().to(device)
    vocoder = bundle.get_vocoder().to(device)

    text = "Hello world! Text to speech!"

    with torch.inference_mode():
      processed, lengths = processor(text)
      processed = processed.to(device)
      lengths = lengths.to(device)
      spec, spec_lengths, _ = tacotron2.infer(processed, lengths)
      waveforms, lengths = vocoder(spec, spec_lengths)

    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))
    ax1.imshow(spec[0].cpu().detach())
    ax2.plot(waveforms[0].cpu().detach())

    torchaudio.save("output_wavernn.wav", waveforms[0:1].cpu(), sample_rate=vocoder.sample_rate)
    IPython.display.display(IPython.display.Audio("output_wavernn.wav"))





.. image-sg:: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_003.png
   :alt: tacotron2 pipeline tutorial
   :srcset: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading: "https://download.pytorch.org/torchaudio/models/wavernn_10k_epochs_8bits_ljspeech.pth" to /root/.cache/torch/hub/checkpoints/wavernn_10k_epochs_8bits_ljspeech.pth
      0%|          | 0.00/16.7M [00:00<?, ?B/s]     51%|#####1    | 8.50M/16.7M [00:00<00:00, 88.8MB/s]    100%|##########| 16.7M/16.7M [00:00<00:00, 88.4MB/s]
    <IPython.lib.display.Audio object>




.. GENERATED FROM PYTHON SOURCE LINES 277-283

Griffin-Lim
~~~~~~~~~~~

Using the Griffin-Lim vocoder is same as WaveRNN. You can instantiate
the vocode object with ``get_vocoder`` method and pass the spectrogram.


.. GENERATED FROM PYTHON SOURCE LINES 283-305

.. code-block:: default


    bundle = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH

    processor = bundle.get_text_processor()
    tacotron2 = bundle.get_tacotron2().to(device)
    vocoder = bundle.get_vocoder().to(device)

    with torch.inference_mode():
      processed, lengths = processor(text)
      processed = processed.to(device)
      lengths = lengths.to(device)
      spec, spec_lengths, _ = tacotron2.infer(processed, lengths)
    waveforms, lengths = vocoder(spec, spec_lengths)

    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))
    ax1.imshow(spec[0].cpu().detach())
    ax2.plot(waveforms[0].cpu().detach())

    torchaudio.save("output_griffinlim.wav", waveforms[0:1].cpu(), sample_rate=vocoder.sample_rate)
    IPython.display.display(IPython.display.Audio("output_griffinlim.wav"))





.. image-sg:: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_004.png
   :alt: tacotron2 pipeline tutorial
   :srcset: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading: "https://download.pytorch.org/torchaudio/models/tacotron2_english_phonemes_1500_epochs_ljspeech.pth" to /root/.cache/torch/hub/checkpoints/tacotron2_english_phonemes_1500_epochs_ljspeech.pth
      0%|          | 0.00/107M [00:00<?, ?B/s]      7%|7         | 7.75M/107M [00:00<00:01, 81.2MB/s]     14%|#4        | 15.5M/107M [00:00<00:01, 80.2MB/s]     22%|##1       | 23.5M/107M [00:00<00:01, 81.7MB/s]     29%|##9       | 31.3M/107M [00:00<00:01, 77.7MB/s]     36%|###6      | 38.7M/107M [00:00<00:00, 75.8MB/s]     43%|####2     | 46.0M/107M [00:00<00:00, 68.5MB/s]     49%|####8     | 52.6M/107M [00:00<00:00, 64.4MB/s]     55%|#####5    | 59.3M/107M [00:00<00:00, 65.9MB/s]     63%|######2   | 67.6M/107M [00:00<00:00, 71.9MB/s]     70%|#######   | 75.5M/107M [00:01<00:00, 74.9MB/s]     78%|#######7  | 83.5M/107M [00:01<00:00, 77.3MB/s]     85%|########4 | 90.9M/107M [00:01<00:00, 77.1MB/s]     92%|#########2| 99.2M/107M [00:01<00:00, 79.9MB/s]     99%|#########9| 107M/107M [00:01<00:00, 79.7MB/s]     100%|##########| 107M/107M [00:01<00:00, 75.4MB/s]
    <IPython.lib.display.Audio object>




.. GENERATED FROM PYTHON SOURCE LINES 306-313

Waveglow
~~~~~~~~

Waveglow is a vocoder published by Nvidia. The pretrained weight is
publishe on Torch Hub. One can instantiate the model using ``torch.hub``
module.


.. GENERATED FROM PYTHON SOURCE LINES 313-334

.. code-block:: default


    # Workaround to load model mapped on GPU
    # https://stackoverflow.com/a/61840832
    waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32', pretrained=False)
    checkpoint = torch.hub.load_state_dict_from_url('https://api.ngc.nvidia.com/v2/models/nvidia/waveglowpyt_fp32/versions/1/files/nvidia_waveglowpyt_fp32_20190306.pth', progress=False, map_location=device)
    state_dict = {key.replace("module.", ""): value for key, value in checkpoint["state_dict"].items()}

    waveglow.load_state_dict(state_dict)
    waveglow = waveglow.remove_weightnorm(waveglow)
    waveglow = waveglow.to(device)
    waveglow.eval()

    with torch.no_grad():
      waveforms = waveglow.infer(spec)

    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))
    ax1.imshow(spec[0].cpu().detach())
    ax2.plot(waveforms[0].cpu().detach())

    torchaudio.save("output_waveglow.wav", waveforms[0:1].cpu(), sample_rate=22050)
    IPython.display.display(IPython.display.Audio("output_waveglow.wav"))



.. image-sg:: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_005.png
   :alt: tacotron2 pipeline tutorial
   :srcset: /auto_examples/tts/images/sphx_glr_tacotron2_pipeline_tutorial_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading: "https://github.com/NVIDIA/DeepLearningExamples/archive/torchhub.zip" to /root/.cache/torch/hub/torchhub.zip
    /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/SpeechSynthesis/Tacotron2/waveglow/model.py:55: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
    The boolean parameter 'some' has been replaced with a string parameter 'mode'.
    Q, R = torch.qr(A, some)
    should be replaced with
    Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1949.)
      W = torch.qr(torch.FloatTensor(c, c).normal_())[0]
    Downloading: "https://api.ngc.nvidia.com/v2/models/nvidia/waveglowpyt_fp32/versions/1/files/nvidia_waveglowpyt_fp32_20190306.pth" to /root/.cache/torch/hub/checkpoints/nvidia_waveglowpyt_fp32_20190306.pth
    <IPython.lib.display.Audio object>





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  16.237 seconds)


.. _sphx_glr_download_auto_examples_tts_tacotron2_pipeline_tutorial.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tacotron2_pipeline_tutorial.py <tacotron2_pipeline_tutorial.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tacotron2_pipeline_tutorial.ipynb <tacotron2_pipeline_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
